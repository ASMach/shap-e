{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964ccced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import platform\n",
    "\n",
    "from shap_e.diffusion.sample import sample_latents\n",
    "from shap_e.diffusion.gaussian_diffusion import diffusion_from_config\n",
    "from shap_e.models.download import load_model, load_config\n",
    "from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget\n",
    "from shap_e.util.image_util import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17365782-258a-469a-9ec7-2e62f60e5274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_device():\n",
    "    if platform.system() == 'Darwin':\n",
    "        print(f\"Torch MPS Available: {torch.backends.mps.is_available()}\")\n",
    "        print(f\"Torch MPS Built: {torch.backends.mps.is_built()}\")\n",
    "    else:\n",
    "        print(torch.cuda.is_available())\n",
    "        print(f\"CUDA Devides: {torch.cuda.device_count()}\")\n",
    "        print(f\"Current CUDA Index: {torch.cuda.current_device()}\")\n",
    "\n",
    "    device = None\n",
    "\n",
    "    if platform.system() == 'Darwin':\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cuda\" if (torch.cude.is_available()) else 'cpu')\n",
    "\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eed3a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch MPS Available: True\n",
      "Torch MPS Built: True\n"
     ]
    }
   ],
   "source": [
    "device = check_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d922637",
   "metadata": {},
   "outputs": [],
   "source": [
    "xm = load_model('transmitter', device=device)\n",
    "model = load_model('image300M', device=device)\n",
    "diffusion = diffusion_from_config(load_config('diffusion'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53d329d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca98f570110f4a4993c83a1b521d7cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# To get the best result, you should remove the background and show only the object of interest to the model.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m image \u001b[38;5;241m=\u001b[39m load_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_data/corgi.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m latents \u001b[38;5;241m=\u001b[39m sample_latents(\n\u001b[1;32m      8\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     10\u001b[0m     diffusion\u001b[38;5;241m=\u001b[39mdiffusion,\n\u001b[1;32m     11\u001b[0m     guidance_scale\u001b[38;5;241m=\u001b[39mguidance_scale,\n\u001b[1;32m     12\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(images\u001b[38;5;241m=\u001b[39m[image] \u001b[38;5;241m*\u001b[39m batch_size),\n\u001b[1;32m     13\u001b[0m     progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     clip_denoised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m     use_fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m     use_karras\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m     karras_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     18\u001b[0m     sigma_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m,\n\u001b[1;32m     19\u001b[0m     sigma_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m160\u001b[39m,\n\u001b[1;32m     20\u001b[0m     s_churn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     21\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/shap-e/shap_e/diffusion/sample.py:128\u001b[0m, in \u001b[0;36msample_latents\u001b[0;34m(batch_size, model, diffusion, model_kwargs, guidance_scale, clip_denoised, use_fp16, use_karras, karras_steps, sigma_min, sigma_max, s_churn, device, progress)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m sample_helper(batch_size,\n\u001b[1;32m    113\u001b[0m                              model,\n\u001b[1;32m    114\u001b[0m                              diffusion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m                              progress,\n\u001b[1;32m    126\u001b[0m                              sample_shape)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sample_helper(batch_size,\n\u001b[1;32m    129\u001b[0m                              model,\n\u001b[1;32m    130\u001b[0m                              diffusion,\n\u001b[1;32m    131\u001b[0m                              model_kwargs,\n\u001b[1;32m    132\u001b[0m                              guidance_scale,\n\u001b[1;32m    133\u001b[0m                              clip_denoised,\n\u001b[1;32m    134\u001b[0m                              use_fp16,\n\u001b[1;32m    135\u001b[0m                              use_karras,\n\u001b[1;32m    136\u001b[0m                              karras_steps,\n\u001b[1;32m    137\u001b[0m                              sigma_min,\n\u001b[1;32m    138\u001b[0m                              sigma_max,\n\u001b[1;32m    139\u001b[0m                              s_churn,\n\u001b[1;32m    140\u001b[0m                              device,\n\u001b[1;32m    141\u001b[0m                              progress,\n\u001b[1;32m    142\u001b[0m                              sample_shape)\n",
      "File \u001b[0;32m~/Documents/GitHub/shap-e/shap_e/diffusion/sample.py:49\u001b[0m, in \u001b[0;36msample_helper\u001b[0;34m(batch_size, model, diffusion, model_kwargs, guidance_scale, clip_denoised, use_fp16, use_karras, karras_steps, sigma_min, sigma_max, s_churn, device, progress, sample_shape)\u001b[0m\n\u001b[1;32m     47\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_karras:\n\u001b[0;32m---> 49\u001b[0m     samples \u001b[38;5;241m=\u001b[39m karras_sample(\n\u001b[1;32m     50\u001b[0m         diffusion\u001b[38;5;241m=\u001b[39mdiffusion,\n\u001b[1;32m     51\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     52\u001b[0m         shape\u001b[38;5;241m=\u001b[39msample_shape,\n\u001b[1;32m     53\u001b[0m         steps\u001b[38;5;241m=\u001b[39mkarras_steps,\n\u001b[1;32m     54\u001b[0m         clip_denoised\u001b[38;5;241m=\u001b[39mclip_denoised,\n\u001b[1;32m     55\u001b[0m         model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[1;32m     56\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     57\u001b[0m         sigma_min\u001b[38;5;241m=\u001b[39msigma_min,\n\u001b[1;32m     58\u001b[0m         sigma_max\u001b[38;5;241m=\u001b[39msigma_max,\n\u001b[1;32m     59\u001b[0m         s_churn\u001b[38;5;241m=\u001b[39ms_churn,\n\u001b[1;32m     60\u001b[0m         guidance_scale\u001b[38;5;241m=\u001b[39mguidance_scale,\n\u001b[1;32m     61\u001b[0m         progress\u001b[38;5;241m=\u001b[39mprogress,\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     internal_batch_size \u001b[38;5;241m=\u001b[39m batch_size\n",
      "File \u001b[0;32m~/Documents/GitHub/shap-e/shap_e/diffusion/k_diffusion.py:113\u001b[0m, in \u001b[0;36mkarras_sample\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkarras_sample\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    112\u001b[0m     last \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m karras_sample_progressive(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m         last \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m last\n",
      "File \u001b[0;32m~/Documents/GitHub/shap-e/shap_e/diffusion/k_diffusion.py:181\u001b[0m, in \u001b[0;36mkarras_sample_progressive\u001b[0;34m(diffusion, model, shape, steps, clip_denoised, progress, model_kwargs, device, sigma_min, sigma_max, rho, sampler, s_churn, s_tmin, s_tmax, s_noise, guidance_scale)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     guided_denoiser \u001b[38;5;241m=\u001b[39m denoiser\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m sample_fn(\n\u001b[1;32m    182\u001b[0m     guided_denoiser,\n\u001b[1;32m    183\u001b[0m     x_T,\n\u001b[1;32m    184\u001b[0m     sigmas,\n\u001b[1;32m    185\u001b[0m     progress\u001b[38;5;241m=\u001b[39mprogress,\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msampler_args,\n\u001b[1;32m    187\u001b[0m ):\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(diffusion, GaussianDiffusion):\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m diffusion\u001b[38;5;241m.\u001b[39munscale_out_dict(obj)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/shap-e/shap_e/diffusion/k_diffusion.py:265\u001b[0m, in \u001b[0;36msample_heun\u001b[0;34m(denoiser, x, sigmas, progress, s_churn, s_tmin, s_tmax, s_noise)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gamma \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    264\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m eps \u001b[38;5;241m*\u001b[39m (sigma_hat\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m sigmas[i] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m--> 265\u001b[0m denoised \u001b[38;5;241m=\u001b[39m denoiser(x, sigma_hat \u001b[38;5;241m*\u001b[39m s_in)\n\u001b[1;32m    266\u001b[0m d \u001b[38;5;241m=\u001b[39m to_d(x, sigma_hat, denoised)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m: i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigma\u001b[39m\u001b[38;5;124m\"\u001b[39m: sigmas[i], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigma_hat\u001b[39m\u001b[38;5;124m\"\u001b[39m: sigma_hat, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_xstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: denoised}\n",
      "File \u001b[0;32m~/Documents/GitHub/shap-e/shap_e/diffusion/k_diffusion.py:173\u001b[0m, in \u001b[0;36mkarras_sample_progressive.<locals>.guided_denoiser\u001b[0;34m(x_t, sigma)\u001b[0m\n\u001b[1;32m    171\u001b[0m x_t \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat([x_t, x_t], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    172\u001b[0m sigma \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mcat([sigma, sigma], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 173\u001b[0m x_0 \u001b[38;5;241m=\u001b[39m denoiser(x_t, sigma)\n\u001b[1;32m    174\u001b[0m cond_x_0, uncond_x_0 \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39msplit(x_0, \u001b[38;5;28mlen\u001b[39m(x_0) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    175\u001b[0m x_0 \u001b[38;5;241m=\u001b[39m uncond_x_0 \u001b[38;5;241m+\u001b[39m guidance_scale \u001b[38;5;241m*\u001b[39m (cond_x_0 \u001b[38;5;241m-\u001b[39m uncond_x_0)\n",
      "File \u001b[0;32m~/Documents/GitHub/shap-e/shap_e/diffusion/k_diffusion.py:160\u001b[0m, in \u001b[0;36mkarras_sample_progressive.<locals>.denoiser\u001b[0;34m(x_t, sigma)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdenoiser\u001b[39m(x_t, sigma):\n\u001b[0;32m--> 160\u001b[0m     _, denoised \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdenoise(\n\u001b[1;32m    161\u001b[0m         x_t, sigma, clip_denoised\u001b[38;5;241m=\u001b[39mclip_denoised, model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m denoised\n",
      "File \u001b[0;32m~/Documents/GitHub/shap-e/shap_e/diffusion/k_diffusion.py:105\u001b[0m, in \u001b[0;36mGaussianToKarrasDenoiser.denoise\u001b[0;34m(self, x_t, sigmas, clip_denoised, model_kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m t \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m    100\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma_to_t(sigma) \u001b[38;5;28;01mfor\u001b[39;00m sigma \u001b[38;5;129;01min\u001b[39;00m sigmas\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()],\n\u001b[1;32m    101\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mth\u001b[38;5;241m.\u001b[39mlong,\n\u001b[1;32m    102\u001b[0m     device\u001b[38;5;241m=\u001b[39msigmas\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m    103\u001b[0m )\n\u001b[1;32m    104\u001b[0m c_in \u001b[38;5;241m=\u001b[39m append_dims(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (sigmas\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m, x_t\u001b[38;5;241m.\u001b[39mndim)\n\u001b[0;32m--> 105\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion\u001b[38;5;241m.\u001b[39mp_mean_variance(\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, x_t \u001b[38;5;241m*\u001b[39m c_in, t, clip_denoised\u001b[38;5;241m=\u001b[39mclip_denoised, model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_xstart\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GitHub/shap-e/shap_e/diffusion/gaussian_diffusion.py:346\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_mean_variance\u001b[0;34m(self, model, x, t, clip_denoised, denoised_fn, model_kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m     model_variance \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mexp(model_log_variance)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 346\u001b[0m     min_log \u001b[38;5;241m=\u001b[39m _extract_into_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposterior_log_variance_clipped, t, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    347\u001b[0m     max_log \u001b[38;5;241m=\u001b[39m _extract_into_tensor(np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbetas), t, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# The model_var_values is [-1, 1] for [min_var, max_var].\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/shap-e/shap_e/diffusion/gaussian_diffusion.py:1068\u001b[0m, in \u001b[0;36m_extract_into_tensor\u001b[0;34m(arr, timesteps, broadcast_shape)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_extract_into_tensor\u001b[39m(arr, timesteps, broadcast_shape):\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;124;03m    Extract values from a 1-D numpy array for a batch of indices.\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;124;03m    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1068\u001b[0m     res \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mfrom_numpy(arr)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mtimesteps\u001b[38;5;241m.\u001b[39mdevice)[timesteps]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   1069\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(broadcast_shape):\n\u001b[1;32m   1070\u001b[0m         res \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "guidance_scale = 3.0\n",
    "\n",
    "# To get the best result, you should remove the background and show only the object of interest to the model.\n",
    "image = load_image(\"example_data/corgi.png\")\n",
    "\n",
    "latents = sample_latents(\n",
    "    batch_size=batch_size,\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    guidance_scale=guidance_scale,\n",
    "    model_kwargs=dict(images=[image] * batch_size),\n",
    "    progress=True,\n",
    "    clip_denoised=True,\n",
    "    use_fp16=True,\n",
    "    use_karras=True,\n",
    "    karras_steps=64,\n",
    "    sigma_min=1e-3,\n",
    "    sigma_max=160,\n",
    "    s_churn=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633da2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_mode = 'nerf' # you can change this to 'stf' for mesh rendering\n",
    "size = 64 # this is the size of the renders; higher values take longer to render.\n",
    "\n",
    "cameras = create_pan_cameras(size, device)\n",
    "for i, latent in enumerate(latents):\n",
    "    images = decode_latent_images(xm, latent, cameras, rendering_mode=render_mode)\n",
    "    display(gif_widget(images))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
