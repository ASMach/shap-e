{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5450e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from shap_e.models.download import load_model\n",
    "from shap_e.util.device import check_device\n",
    "from shap_e.util.data_util import load_or_create_multimodal_batch\n",
    "from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6217aaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch MPS Available: True\n",
      "Torch MPS Built: True\n"
     ]
    }
   ],
   "source": [
    "device = check_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba1fe89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a728bab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alonzomachiraju/Documents/GitHub/shap-e/shap_e/models/nn/checkpoint.py:31: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/Users/alonzomachiraju/Documents/GitHub/shap-e/shap_e/models/nn/checkpoint.py:43: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/Users/alonzomachiraju/Documents/GitHub/shap-e/shap_e/models/nn/checkpoint.py:61: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/Users/alonzomachiraju/Documents/GitHub/shap-e/shap_e/models/nn/checkpoint.py:86: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/Users/alonzomachiraju/Documents/GitHub/shap-e/shap_e/models/download.py:136: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "xm = load_model('transmitter', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb7d0f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating point cloud...\n",
      "creating multiview...\n"
     ]
    }
   ],
   "source": [
    "model_path = \"example_data/cactus/object.obj\"\n",
    "\n",
    "# This may take a few minutes, since it requires rendering the model twice\n",
    "# in two different modes.\n",
    "batch = load_or_create_multimodal_batch(\n",
    "    device,\n",
    "    model_path=model_path,\n",
    "    mv_light_mode=\"basic\",\n",
    "    mv_image_size=256,\n",
    "    cache_dir=\"example_data/cactus/cached\",\n",
    "    verbose=True, # this will show Blender output during renders\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15aba0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alonzomachiraju/Documents/GitHub/shap-e/shap_e/rendering/mc.py:37: UserWarning: The operator 'aten::__lshift__.Scalar' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  bitmasks = bitmasks[:-1, :, :] | (bitmasks[1:, :, :] << 1)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'raw_meshes' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m \u001b[38;5;66;03m# recommended that you lower resolution when using nerf\u001b[39;00m\n\u001b[1;32m      7\u001b[0m cameras \u001b[38;5;241m=\u001b[39m create_pan_cameras(size, device)\n\u001b[0;32m----> 8\u001b[0m images \u001b[38;5;241m=\u001b[39m decode_latent_images(xm, latent, cameras, rendering_mode\u001b[38;5;241m=\u001b[39mrender_mode)\n\u001b[1;32m      9\u001b[0m display(gif_widget(images))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/GitHub/shap-e/shap_e/util/notebooks.py:53\u001b[0m, in \u001b[0;36mdecode_latent_images\u001b[0;34m(xm, latent, cameras, rendering_mode)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_latent_images\u001b[39m(\n\u001b[1;32m     48\u001b[0m     xm: Union[Transmitter, VectorDecoder],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     rendering_mode: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     52\u001b[0m ):\n\u001b[0;32m---> 53\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m xm\u001b[38;5;241m.\u001b[39mrenderer\u001b[38;5;241m.\u001b[39mrender_views(\n\u001b[1;32m     54\u001b[0m         AttrDict(cameras\u001b[38;5;241m=\u001b[39mcameras),\n\u001b[1;32m     55\u001b[0m         params\u001b[38;5;241m=\u001b[39m(xm\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(xm, Transmitter) \u001b[38;5;28;01melse\u001b[39;00m xm)\u001b[38;5;241m.\u001b[39mbottleneck_to_params(\n\u001b[1;32m     56\u001b[0m             latent[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     57\u001b[0m         ),\n\u001b[1;32m     58\u001b[0m         options\u001b[38;5;241m=\u001b[39mAttrDict(rendering_mode\u001b[38;5;241m=\u001b[39mrendering_mode, render_with_direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     59\u001b[0m     )\n\u001b[1;32m     60\u001b[0m     arr \u001b[38;5;241m=\u001b[39m decoded\u001b[38;5;241m.\u001b[39mchannels\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39muint8)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [Image\u001b[38;5;241m.\u001b[39mfromarray(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arr]\n",
      "File \u001b[0;32m~/Documents/GitHub/shap-e/shap_e/models/nerstf/renderer.py:243\u001b[0m, in \u001b[0;36mNeRSTFRenderer.render_views\u001b[0;34m(self, batch, params, options)\u001b[0m\n\u001b[1;32m    233\u001b[0m         sdf_fn \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m    234\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msdf\u001b[38;5;241m.\u001b[39mforward_batched,\n\u001b[1;32m    235\u001b[0m             params\u001b[38;5;241m=\u001b[39msubdict(params, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdf\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    236\u001b[0m             options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m         tf_fn \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m    239\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf\u001b[38;5;241m.\u001b[39mforward_batched,\n\u001b[1;32m    240\u001b[0m             params\u001b[38;5;241m=\u001b[39msubdict(params, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    241\u001b[0m             options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    242\u001b[0m         )\n\u001b[0;32m--> 243\u001b[0m     output \u001b[38;5;241m=\u001b[39m render_views_from_stf(\n\u001b[1;32m    244\u001b[0m         batch,\n\u001b[1;32m    245\u001b[0m         options,\n\u001b[1;32m    246\u001b[0m         sdf_fn\u001b[38;5;241m=\u001b[39msdf_fn,\n\u001b[1;32m    247\u001b[0m         tf_fn\u001b[38;5;241m=\u001b[39mtf_fn,\n\u001b[1;32m    248\u001b[0m         nerstf_fn\u001b[38;5;241m=\u001b[39mnerstf_fn,\n\u001b[1;32m    249\u001b[0m         volume\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvolume,\n\u001b[1;32m    250\u001b[0m         grid_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_size,\n\u001b[1;32m    251\u001b[0m         channel_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_scale,\n\u001b[1;32m    252\u001b[0m         texture_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexture_channels,\n\u001b[1;32m    253\u001b[0m         ambient_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mambient_color,\n\u001b[1;32m    254\u001b[0m         diffuse_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffuse_color,\n\u001b[1;32m    255\u001b[0m         specular_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecular_color,\n\u001b[1;32m    256\u001b[0m         output_srgb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_srgb,\n\u001b[1;32m    257\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m    258\u001b[0m     )\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/shap-e/shap_e/models/stf/renderer.py:242\u001b[0m, in \u001b[0;36mrender_views_from_stf\u001b[0;34m(batch, options, sdf_fn, tf_fn, nerstf_fn, volume, grid_size, channel_scale, texture_channels, ambient_color, diffuse_color, specular_color, output_srgb, device)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type, enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    241\u001b[0m         mask_helper()\n\u001b[0;32m--> 242\u001b[0m max_vertices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(m\u001b[38;5;241m.\u001b[39mverts) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m raw_meshes)\n\u001b[1;32m    244\u001b[0m fn \u001b[38;5;241m=\u001b[39m nerstf_fn \u001b[38;5;28;01mif\u001b[39;00m tf_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tf_fn\n\u001b[1;32m    245\u001b[0m tf_out \u001b[38;5;241m=\u001b[39m fn(\n\u001b[1;32m    246\u001b[0m     query\u001b[38;5;241m=\u001b[39mQuery(\n\u001b[1;32m    247\u001b[0m         position\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    254\u001b[0m )\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'raw_meshes' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    latent = xm.encoder.encode_to_bottleneck(batch)\n",
    "\n",
    "    render_mode = 'stf' # you can change this to 'nerf'\n",
    "    size = 128 # recommended that you lower resolution when using nerf\n",
    "\n",
    "    cameras = create_pan_cameras(size, device)\n",
    "    images = decode_latent_images(xm, latent, cameras, rendering_mode=render_mode)\n",
    "    display(gif_widget(images))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
